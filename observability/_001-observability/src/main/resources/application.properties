spring.application.name=Observability

# Ollama connections properties
spring.ai.ollama.base-url=${OLLAMA_URL:http://localhost:11434}
spring.ai.ollama.chat.options.model=qwen2.5:3b
#spring.ai.ollama.chat.options.model=llama3.1:8b
#spring.ai.ollama.chat.options.model=llama3.2:1b
#spring.ai.ollama.chat.options.model=deepseek-r1:7b
spring.ai.ollama.embedding.options.model=nomic-embed-text:latest

management.endpoints.web.exposure.include=*
management.endpoint.health.show-details=always

server.port=8081
spring.threads.virtual.enable=true
debug=true